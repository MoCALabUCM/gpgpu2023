<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>GPGPU 2022</title>
	<meta charset="utf-8" />
	<meta name="viewport"
		content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">
			<div class="inner">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo"><strong>GPGPU</strong>
						2022</a>
					<!-- <ul class="icons">
						<li><a href="#" class="icon brands fa-twitter"><span
									class="label">Twitter</span></a></li>
						<li><a href="#" class="icon brands fa-facebook-f"><span
									class="label">Facebook</span></a></li>
						<li><a href="#"
								class="icon brands fa-snapchat-ghost"><span
									class="label">Snapchat</span></a></li>
						<li><a href="#" class="icon brands fa-instagram"><span
									class="label">Instagram</span></a></li>
						<li><a href="#" class="icon brands fa-medium-m"><span
									class="label">Medium</span></a></li>
					</ul> -->
				</header>

				<!-- Banner -->
				<section id="banner">
					<div class="content">
						<header>
							<h1>GPGPU 2022</h1>
							<p>The 14th Workshop on General Purpose Processing
								Using GPU (GPGPU 2022)</p>
							<p>April 3, 2022, Online</p>
							<p>Zoom Link: <a
									href="https://acm-org.zoom.us/j/96884634475?pwd=d1hCeFc1VFhOMnNjZnk4Z05UQ0ZwZz09">https://acm-org.zoom.us/j/96884634475?pwd=d1hCeFc1VFhOMnNjZnk4Z05UQ0ZwZz09</a>
							</p>
						</header>
						<p class="rmargin" align="justify">
							Massively parallel (GPUs and other data-parallel
							accelerators) devices are delivering more and more
							computing powers required by modern society. With
							the growing popularity of massively parallel
							devices, users demand better performance,
							programmability, reliability, and security. The
							goal of this workshop is to provide a forum to
							discuss massively parallel applications,
							environments, platforms, and architectures, as well
							as infrastructures that facilitate related
							research. This year, we are no longer limited to
							GPU applications and architectures. We welcome
							research related to any highly parallel computing
							accelerators and devices. <br />


							Authors are invited to
							submit original research papers in the general area
							of massively parallel computing and architectures.
							Topics include, but are not limited to:
						</p>
						<ul class="">
							<li>Security for GPU architecture and other
								accelerators</li>
							<li>AR/VR support using GPUs or other accelerators
							</li>
							<li>Heterogeneous systems</li>
							<li>Cloud-based GPU computing</li>
							<li>Serverless/disaggregated GPU computing</li>
							<li>GPU/accelerator virtualization/containerization
							</li>
							<li>GPU applications</li>
							<li>GPU performance evaluation/benchmarking</li>
							<li>GPU programming languages</li>
							<li>Operating system support for GPU execution</li>
							<li>GPU compilation techniques </li>
							<li>GPU reliability</li>
							<li>GPU hardware architecture for graphics and
								general-purpose applications</li>
							<li>Power-constrained GPU techniques</li>
							<li>Multi-GPU systems</li>
							<li>Network system design for intra- and
								inter-accelerator communication</li>
							<li>Domain-specific accelerators</li>
							<li>Research & design tools for GPU development
							</li>

						</ul>
					</div>
					<!-- <span class="image object"> banner image
						<img src="images/pic10.jpg" alt="" />
					</span> -->
				</section>

				<!-- Section -->
				<section>
					<header class="major" id="program">
						<h2>Workshop Program</h2>
					</header>
					<div>
						<h3>All times are in US Eastern time (UTC-4). </h3>
						<table class="tg"
							style="table-layout: fixed; width: 100%">
							<colgroup>
								<col style="width: 160px">
								<col style="width: 730px">
							</colgroup>
							<!-- <tr style="border-bottom: 1px solid #000;border-top: 1px solid #000;">
								<td class="tg-fymr">07:00 AM - 09:00 AM</td>
								<td class="tg-0pky">Breakfast</td>
							  </tr> -->
							<tr
								style="border-bottom: 1px solid #000;border-top: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">10:00 AM -
										10:10 AM</span></td>
								<td class="tg-0pky">
									Opening Remarks
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/BOZGCt6CxN0"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-0pky"><span
										style="font-weight:bold">10:10 AM -
										11:10 AM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Keynote]</span>
									<span style="font-style:italic">Building
										Performant and Portable Heterogenous
										Code using GPU Compute
										Accelerators</span> <br>Derek Bouius,
									AMD
									<a href="#keynote">[Link]</a>
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>This talk will describe the
												various
												methodologies used to offload
												computationally intensive
												workloads
												from CPUs to accelerators. Key
												topics will cover HW
												architecture
												considerations and programming
												methodologies along with
												debugging
												and profiling techniques.</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/00Dnxi4w5uM"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-0pky"><span
										style="font-weight:bold">11:10 AM -
										11:30 AM</span></td>
								<td class="tg-0pky">Break</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td></td>
								<td></td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr" style="font-weight:bold">
									Session 1</td>
								<td>Session Chair: Daniel Wong</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">11:30 AM -
										11:50 AM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Paper]</span>
									<span style="font-style:italic">Near LLC
										Versus Near Main Memory
										Processing</span><br>Hossein Bitalebi,
									Vahid Geraeinejad, Masoumeh Ebrahimi (KTH
									Royal Institute of Technology)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>Emerging advanced applications,
												such as deep learning and
												graphprocessing, with enormous
												processing demand and massive
												mem-ory requests call for a
												comprehensive processing system
												or ad-vanced solutions to
												address these requirements.
												Near data process-ing is one of
												the promising structures
												targeting this goal.
												However,most recent studies
												have focused on processing
												instructions nearthe main
												memory data banks while
												ignoring the benefits of
												pro-cessing instructions near
												other memory hierarchy levels
												such as LLC. In this study, we
												investigate the near LLC
												processing struc-tures, and
												compare it to the near main
												memory processing alter-native,
												specifically in graphical
												processing units. We analyze
												thesetwo structures on various
												applications in terms of
												performance andpower. Results
												show a clear benefit of near
												LLC processing overnear main
												memory processing in a class of
												applications. Further,we
												suggest a structure, which
												could benefit from both
												structures,requiring the
												applications to be
												characterized in advance or at
												runtime.</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/S7TuzCKK0W0"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">11:50 AM -
										12:10 PM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Paper]</span>
									<span
										style="font-style:italic">Accelerating
										Data Transfer between Host and Device
										using Idle GPU</span><br>Yuya Tatsugi,
									Akira Nukada (University of Tsukuba)<br>
									<!-- <br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>When running single-GPU
												applications on multi-GPU
												compute nodes, the remaining
												GPU devices are kept idle. We
												propose a novel technology to
												accelerate these single-GPU
												applications using the idle GPU
												devices. The data transfers
												between host and device are
												performed not only by the first
												GPU but also by the second GPU
												as well as the alternative
												route with PCI-Express and
												NV-Link connected to it. Our
												performance evaluations show
												the proposed method enables
												about twice data transfer speed
												as native single GPU case for
												large data sizes.</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/7aaO4LJcQz8"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>

							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">12:10 PM -
										12:30 PM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Invited
										Talk]</span> <span
										style="font-style:italic"> Towards True
										Coherent Shared Memory for
										Next-generation Multi-GPU
										Systems</span><br>José L. Abellán
									(Universidad Católica de Murcia)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>Multi-GPU (MGPU) systems are
												commonly used today to
												accelerate a variety of
												workloads, including machine
												learning applications, graph
												applications, and large-scale
												simulations. However, the
												inefficiencies in terms of NUMA
												effects and difficulties in
												programming due to the lack of
												hardware coherence in these
												MGPU systems call for new
												architectural solutions for
												MGPU systems. To address these
												inefficiencies, in this talk I
												will present the first proposal
												of a true shared main memory
												(TSM) for MGPU systems, and
												then, to enable seamless
												sharing of data in an MGPU
												system with TSM (MGPU-TSM), I
												will introduce a novel
												lightweight scalable
												timestamp-based coherence
												protocol called MGCC. For
												standard benchmarks, an
												MGPU-TSM system (with 4 GPUs,
												using MGCC) implemented using
												the MGPUSim simulator performs
												on average, 3.7x and 3.0x
												better with relaxed and
												sequential consistency,
												respectively, than the
												non-coherent conventional MGPU
												systems with the same number of
												GPUs. In addition, compared to
												a coherent MGPU system using
												the state-of-the-art HMG
												coherence protocol, an MGPU
												system using MGCC achieves 2.4x
												higher performance. Finally, I
												will discuss some of the key
												challenges and open research
												directions to further optimize
												an MGPU-TSM system. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/wF_5RZZlebI"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">12:30 PM -
										12:50 PM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Invited
										Talk]</span>
									<span style="font-style:italic">Re-design
										GPU NoC and LLC
										System</span>
									<br>Xia Zhao (Academy
									of Military Sciences)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>To provide high compute
												power, GPUs feature an
												increased number of SMs
												with a larger LLC size and
												higher memory bandwidth.
												Facing the new
												opportunities and
												challenges, how to design a
												scalable and
												high-performance NoC and
												LLC system becomes
												especially important. In
												this talk, I will introduce
												our recent work including
												hierarchy NoC design for
												GPUs, adaptive memory-side
												last-level GPU caching, and
												selective replication in
												memory-side GPU caches.
												The hierarchy NoC provides
												a scalable and low-cost
												interconnect network to
												connect the SMs with the
												LLCs and memory controllers
												by fully exploiting the
												unique traffic pattern in
												GPUs. To solve the
												contention caused by the
												concurrent memory accesses
												sent to the same shared
												data, adaptive LLC can
												adaptively choose shared
												LLC or private LLC based on
												the application
												characteristics. Compared
												to the coarse grain of data
												replication in adaptive
												LLC, selective replication
												can selectively choose the
												replication degree to
												reduce data contention
												while avoiding the high LLC
												miss rate caused by the
												duplicated data. All these
												designs significantly
												increase GPU performance
												for data-intensive
												applications. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/0Lm9URF90co"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">12:50 PM -
										01:10 PM</span></td>
								<td class="tg-0pky">Break</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td></td>
								<td></td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr" style="font-weight:bold">
									Session 2</td>
								<td>Session Chair: Hoda NaghibiJouybari</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">01:10 PM -
										01:30 PM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Paper]</span>
									<span
										style="font-style:italic">Systematically
										Extending a High-Level CodeGenerator
										with Support for Tensor Cores</span>
									<br>Lukas Siefke, Bastian Köpcke
									(University of Münster), Michel Steuwer
									(University of Edinburgh), Sergei Gorlatch
									(University of Muenster)
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>High-level code generators like
												Halide, Lift, and RISE make a
												compelling proposition: write
												programs in a simple high-level
												language and get
												high-performing GPU code “for
												free”. They achieve this feat
												by restricting the input
												language to a specific domain
												(such as image and array
												processing in Halide) or to a
												fixed set of flexible parallel
												patterns (as Lift and RISE do).
												Implementing high-level code
												generators that produce
												high-performance code is
												challenging, specifically as
												the target hardware constantly
												evolves.
												In this paper, we discuss how
												we systematically extend the
												RISE high-level code generator
												with support for tensor cores,
												a specialized hardware feature
												of recent Nvidia GPUs. We
												highlight the design of RISE
												that makes it easily extensible
												by following a systematic
												bottom-up approach, that first,
												exposes the imperative tensor
												core API to the code generator,
												then, raises the abstractions
												to an internal low-level
												functional representation,
												that, finally, is targeted by a
												rewrite process that starts
												from a high-level functional
												program.
												Our experimental evaluation
												shows that RISE with support
												for tensor cores generates code
												of competitive performance to
												manually optimized CUDA code,
												which is only up to 36%, but on
												average only 10%, slower than
												Nvidia’s highly optimized
												cuBLAS library, and clearly
												outperforms any code that does
												not exploit tensor cores. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/8IM0sFb_lvw"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>

							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">01:30 PM -
										01:50 PM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Paper]</span>
									<span
										style="font-style:italic">Compiler-Assisted
										Scheduling for Multi-Instance
										GPUs</span> <br>Chris Porter (Georgia
									Institute of Technology), Chao Chen (Amazon
									Web Service), Santosh Pande (Georgia
									Institute of Technology)
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>NVIDIA's Multi-Instance GPU
												(MIG) feature allows users to
												partition a GPU's compute and
												memory into independent
												hardware instances. MIG
												guarantees full isolation among
												co-executing kernels on the
												device, which boosts security
												and prevents
												interference-related
												performance degradation.
												Despite the benefits of
												isolation, however, certain
												workloads do not necessarily
												need such guarantees,
												and in fact enforcing such
												isolation can negatively impact
												the throughput of a
												group of processes. In this
												work we aim to relax the
												isolation property for
												certain types of jobs, and to
												show how this can dramatically
												boost throughput
												across a mixed workload
												consisting of jobs that demand
												isolation and others
												that do not. The number of MIG
												partitions is hardware-limited
												but configurable,
												and state-of-the-art workload
												managers cannot safely take
												advantage of unused
												and wasted resources inside a
												given partition. We show how a
												compiler and
												runtime system working in
												tandem can be used to pack jobs
												into partitions when
												isolation is not necessary.
												Using this technique we improve
												overall
												utilization of the device while
												still reaping the benefits of
												MIG's isolation
												properties. Our experimental
												results on NVIDIA A30s with a
												throughput-oriented
												workload show an average of
												1.45x throughput improvement
												and 2.93x increase in
												GPU memory utilization over the
												Slurm workload manager. The
												presented
												framework is fully automatic
												and requires no changes to user
												code. Based on
												these results, we believe our
												scheme is a practical and
												strong advancement over
												state-of-the-art techniques
												currently employed for MIG.
											</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/i2jkFAxt9jE"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
								</td>
							</tr>


							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">01:50 PM -
										02:05 PM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Work-in-Progress
										Presentation]</span> <span
										style="font-style:italic"> PTXVM:
										Translating PTX to
										C</span><br>Sreepathi Pai, Benjamin
									Carleton, Benjamin Valpey, Amr Elhelw
									(University of Rochester)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>We describe our ongoing effort
												to translate CUDA PTX kernels
												to C. Our
												translator, PTXVM, generates
												single-threaded C code from
												existing PTX
												kernels and does not need an
												interpreter. PTXVM is
												distinguished by
												its expansive and faithful
												support of NVIDIA's PTX
												specification. This
												enables it to run many complex
												real-life programs such CUB and
												ModernGPU, libraries such as
												cuRAND, and also benchmarks
												such as the
												IrGL graph algorithms, Rodinia,
												and PolyBench. In this talk,
												I'll
												describe the architecture of
												PTXVM as well as an example
												tracing
												infrastructure we've built on
												top of it to gather execution
												statistics. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/aGjZXEEudRE"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">02:05 PM -
										02:20 PM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Work-in-Progress
										Presentation]</span> <span
										style="font-style:italic">Understanding
										Wafer-Scale GPU Performance using an
										Architectural Simulator</span><br>Chris
									Thames, Yifan Sun (William &amp; Mary)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>Wafer-Scale chips have the
												potential to break the die-size
												limitation and provide extreme
												performance scalability.
												Existing solutions have
												demonstrated the possibility of
												integrating multi-CPU and
												multi-GPU systems at a
												significantly larger scale on a
												wafer. This increased
												capability results in an
												increase of complexity in
												managing the memory and
												computing resources. To
												facilitate the community study
												wafer-scale systems, this paper
												develops an architectural
												simulator dedicated to model
												wafer-scale multi-device
												systems. Also, this work
												demonstrates analysis of
												initial results from
												simulations on wafer-scale GPU
												systems, providing useful
												insight that can guide future
												system design. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/1O7oBIy0GoU"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">02:20 PM -
										02:35 PM</span></td>
								<td class="tg-0pky"><span
										style="font-weight:bold">[Work-in-Progress
										Presentation]</span> <span
										style="font-style:italic">ScaleServe: A
										Scalable Multi-GPU Machine Learning
										Inference System and Benchmarking
										Suite</span><br>Ali Jahanshahi, Marcus
									Chow, Daniel Wong (UC Riverside)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content"
											style="display:none">
											<p>We present, SCALESERVE, a
												scalable multi-GPU inference
												system for a variety of machine
												learning tasks. The proposed
												suite is unique in that each
												component of SCALESERVE
												provides the users with
												configuration knobs which can
												be fine-tuned based on the
												specifications of the
												deployment platform to achieve
												the maximum performance for the
												serving. SCALESERVE also
												provides detailed performance
												metrics/statistics from
												different components of the
												server which can be used by
												designers to characterize the
												bottlenecks of the server.
												We evaluate SCALESERVE serving
												scalability with several
												machine learning tasks
												including computer vision and
												natural language processing on
												an 8-GPU server. We used the
												provided statistic by
												SCALESERVE to fine-tune the
												inference server on our target
												platform to achieve maximum
												performance. The performance
												results for ResNet152 show that
												SCALESERVE is able to scale
												well on a multi-GPU platform.
											</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/Aem6TcNZJu8"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span
										style="font-weight:bold">02:35 PM -
										02:40 PM</span></td>
								<td class="tg-0pky">
									Closing Remarks
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content"
											style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/MAqNlciDO34"
												title="YouTube video player"
												frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
						</table>
					</div>
				</section>

				<section id="keynote">
					<header id="header">
						<a class="logo"><strong>
								<font size=6em>Keynotes</font>
							</strong></a>
					</header>
					<!-- </section> -->
					<section id="banner">
						<div class="content">
							<div style="float: left; margin-right: 10px;"><img
									src="images/portraits/derek-gpgpu2022.png"
									width="160" height="180" /> </div>
							<b>Speaker:</b> Derek Bouius, AMD<br />
							<b>Title: Building Performant and Portable
								Heterogenous Code using GPU Compute
								Accelerators </b><br /><br />
							<p align="justify"><b>Abstract:</b> This talk will
								describe the various methodologies used to
								offload computationally intensive workloads
								from CPUs to accelerators. Key topics will
								cover HW architecture considerations and
								programming methodologies along with debugging
								and profiling techniques.</p>
							<p align="justify"><b>Bio:</b> Derek Bouius has
								been leading product management of the AMD ROCm
								open software platform for GPU compute for the
								past 4 years. This open source initiative is
								closely tied to enablement of ML and HPC
								workloads using the newest data center GPU
								devices. Derek has been involved in the design,
								development and deployment of security and
								compute accelerators for over 20 years.</p>
							<br />

						</div>
					</section>


					<section>
						<header class="major" id="importantdates">
							<h2>Important Dates</h2>
						</header>

						<ul>
							<li>
								Papers due:
								<s>January 25, 2022</s>
								February 8, 2022
							</li>
							<li>Notification: March 15, 2022 </li>
							<li>Final paper due: April 2, 2022 </li>
						</ul>


					</section>

					<section>
						<header class="major" id="guidelines">
							<h2>Submission Guidelines</h2>
						</header>
						<p>
							Full paper submissions must be in PDF format for US
							letter-size paper. They must not exceed 6 pages
							(all
							inclusive) in standard ACM two-column conference
							format
							(review mode, with page numbers and both 9 or 10pt
							can
							be used). GPGPU also accepts extended abstracts (2
							pages including references). Authors can select if
							they
							want to reveal their identity in the submission.
							Templates for ACM format are available for
							Microsoft
							Word and LaTeX at: <a
								href="https://drupal.sigplan.org/authorInformation.htm">https://drupal.sigplan.org/authorInformation.htm</a>
						</p>

						<p>
							At least one author must present at the workshop
							conference. Travel func may be applied through
							SIGPLAN
							Professional Activities Committee (PAC). Details
							are
							available <a
								href="https://www.sigplan.org/PAC/">here</a>
						</p>

						<p>
							Submission Site: <a
								href="https://easychair.org/conferences/?conf=gpgpu2022">GPGPU
								2022</a>
						</p>


					</section>

					<section>
						<header class="major" id="organizers">
							<h2>Workshop Organizers</h2>
						</header>

						<div class="content">
							<table style="width:100%">
								<col style="width:25%">
								<col style="width:25%">
								<col style="width:25%">
								<col style="width:25%">
								<tr>
									<td><img src="images/portraits/yifan.jpg"
											style="border-radius: 10px;" alt=""
											data-position="center center"
											width='200' /></td>
									<td><img src="images/portraits/daniel.png"
											style="border-radius: 10px;" alt=""
											data-position="center center"
											width='200' />
									</td>
									<td><img src="images/portraits/hoda.png"
											style="border-radius: 10px;" alt=""
											data-position="center center"
											width='200' /></td>
									<td><img src="images/portraits/hyliu.jpg"
											style="border-radius: 10px;" alt=""
											data-position="center center"
											width='200' /></td>
								</tr>
								<tr>
									<td><a href="https://syifan.github.io/"
											target=&ldquo;blank&rdquo;>Yifan
											Sun</a></td>
									<td><a href="https://www.danielwong.org/"
											target=&ldquo;blank&rdquo;>Daniel
											Wong</a>
									</td>
									<td><a href="https://sites.google.com/a/binghamton.edu/hoda/"
											target=&ldquo;blank&rdquo;>Hoda
											NaghibiJouybari</a></td>
									<td><a href="https://liuhongyuan.com"
											target=&ldquo;blank&rdquo;>Hongyuan
											Liu</a></td>
								</tr>
								<tr>
									<td>Co-chair</td>
									<td>Co-chair</td>
									<td>Co-chair</td>
									<td>Publication/Web Chair</td>
								</tr>
								<tr>
									<td>William &amp; Mary</td>
									<td>UC Riverside</td>
									<td>Binghamton University</td>
									<td>William &amp; Mary</td>
								</tr>
							</table>

							Please contact the organizers if you have any
							questions.
						</div>
					</section>

					<section>
						<header class="major" id="pcmember">
							<h2>Program Committee</h2>
						</header>
						<ul>
							<li>Tor M. Aamodt (University of British Columbia)
							</li>
							<li>José L. Abellán (Universidad Católica de
								Murcia)
							</li>
							<li>Nael Abu-Ghazaleh (University of California,
								Riverside)</li>
							<li>Trinayan Baruah (AMD)</li>
							<li>Zhongliang Chen (AMD)</li>
							<li>Shi Dong (Cerebras)</li>
							<li>Xiang Gong (Qualcomm)</li>
							<li>Hyeran Jeon (University of California, Merced)
							</li>
							<li>Adwait Jog (William & Mary)</li>
							<li>David Kaeli (Northeastern University)</li>
							<li>Onur Kariyan (AMD)</li>
							<li>Gunjae Koo (Korea University)</li>
							<li>Jiajia Li (William & Mary)</li>
							<li>Ashutosh Pattnaik (ARM)</li>
							<li>Seunghee Shin (Binghamton University)</li>
							<li>Jieming Yin (Lehigh University) </li>
							<li>Jishen Zhao (University of California, San
								Diego)
							</li>
							<li>Huiyang Zhou (North Carolina State University)
							</li>
						</ul>
					</section>


					<section>
						<header class="major" id="hist">
							<h2>History and Impact</h2>
						</header>

						<div class="content">
							<div>
								David Kaeli (Northeastern) and John Cavazos
								(Delaware) started this GPGPU workshop series,
								which was first held in 2007 at Northeastern
								University. In 2008, the workshop was held with
								ASPLOS 2008. This trend continued and this
								GPGPU
								workshop was held with ASPLOS for the next
								6 years. From 2015 to 2018, the GPGPU workshop
								was
								co-located with PPoPP.

								In 2019 and 2020, the GPGPU workshop is
								co-hosted
								by Adwait Jog (William & Mary), Onur Kayiran
								(AMD),
								and Ashutosh Pattnaik (ARM).


								The average
								citation count (as per Google Scholar), for a
								GPGPU
								workshop paper is currently 37.5, where
								there have been 8 influential papers with 100+
								citations.
							</div>
							<br />
							<h2>Previous versions of the GPGPU workshop:</h2>
							<ul>
								<li><a href="https://insight-archlab.github.io/gpgpu.html"
										target=&ldquo;blank&rdquo;>GPGPU
										13 (2020)</a> <a
										href="https://dblp.org/db/conf/ppopp/gpgpu2020.html"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://insight-archlab.github.io/gpgpu12.html"
										target=&ldquo;blank&rdquo;>GPGPU 12
										(2019)</a> <a
										href="https://dblp.org/db/conf/asplos/gpgpu2019.html"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://gpgpu11.000webhostapp.com/"
										target=&ldquo;blank&rdquo;>GPGPU 11
										(2018)</a> <a
										href="https://dblp.org/db/conf/ppopp/gpgpu2018"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://gpgpu10.athoura.com/"
										target=&ldquo;blank&rdquo;>GPGPU 10
										(2017)</a> <a
										href="https://dblp.org/db/conf/ppopp/gpgpu2017"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://conf.researchr.org/track/PPoPP-2016/GPGPU-2016-papers"
										target=&ldquo;blank&rdquo;>GPGPU 09
										(2016)</a> <a
										href="https://dblp.org/db/conf/ppopp/gpgpu2016"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU8/"
										target=&ldquo;blank&rdquo;>GPGPU 08
										(2015)</a> <a
										href="https://dblp.org/db/conf/ppopp/gpgpu2015"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU6/"
										target=&ldquo;blank&rdquo;>GPGPU 07
										(2014)</a> <a
										href="https://dblp.org/db/conf/asplos/gpgpu2014"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU6/"
										target=&ldquo;blank&rdquo;>GPGPU 06
										(2013)</a> <a
										href="https://dblp.org/db/conf/asplos/gpgpu2013"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2012"
										target=&ldquo;blank&rdquo;>GPGPU 05
										(2012)</a> <a
										href="https://dblp.org/db/conf/asplos/gpgpu2012"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2011"
										target=&ldquo;blank&rdquo;>GPGPU 04
										(2011)</a> <a
										href="https://dblp.org/db/conf/asplos/gpgpu2011"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2010"
										target=&ldquo;blank&rdquo;>GPGPU 03
										(2010)</a> <a
										href="https://dblp.org/db/conf/asplos/gpgpu2010"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2009"
										target=&ldquo;blank&rdquo;>GPGPU 02
										(2009)</a> <a
										href="https://dblp.org/db/conf/asplos/gpgpu2009"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li>GPGPU 01 (2008)
								</li>
							</ul>
						</div>
					</section>



			</div>
		</div>

		<!-- Sidebar -->
		<div id="sidebar">
			<div class="inner">

				<!-- Menu -->
				<nav id="menu">
					<!-- <header class="major">
						<h2>MENU</h2>
					</header> -->
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="#program">Program</a></li>
						<li><a href="#keynote">Keynotes</a></li>
						<li><a href="#importantdates">Important Dates</a></li>
						<li><a href="#guidelines">Submission Guidelines</a>
						</li>
						<li><a href="#organizers">Workshop Organizers</a></li>
						<li><a href="#pcmember">Program Committee</a></li>
						<li><a href="#">Proceedings</a></li>
						<li><a href="#hist">History and Impact</a></li>

					</ul>
				</nav>
			</div>
		</div>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>