<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>GPGPU 2023</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">
			<div class="inner">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo"><strong>GPGPU</strong>
						2023</a>
					<!-- <ul class="icons">
						<li><a href="#" class="icon brands fa-twitter"><span
									class="label">Twitter</span></a></li>
						<li><a href="#" class="icon brands fa-facebook-f"><span
									class="label">Facebook</span></a></li>
						<li><a href="#"
								class="icon brands fa-snapchat-ghost"><span
									class="label">Snapchat</span></a></li>
						<li><a href="#" class="icon brands fa-instagram"><span
									class="label">Instagram</span></a></li>
						<li><a href="#" class="icon brands fa-medium-m"><span
									class="label">Medium</span></a></li>
					</ul> -->
				</header>

				<!-- Banner -->
				<section id="banner">
					<div class="content">
						<header>
							<h1>GPGPU 2023</h1>
							<p>The 15th Workshop on General Purpose Processing
								Using GPU (GPGPU 2023)</p>
							<!-- place and date -->
							<!-- <p>April 3, 2022, Online</p>   -->
							<p>Feb 25 Full Day, 2023, Montreal, Canada</p>

							<p>Zoom Link: <a
									href="https://ucmerced.zoom.us/j/83338827843?pwd=SlN1UkNNNmpuaEVBWURGSG5wSllSdz09">https://ucmerced.zoom.us/j/83338827843?pwd=SlN1UkNNNmpuaEVBWURGSG5wSllSdz09</a>
							</p> 
							<!-- <p>Zoom Link: <a href="TBD"></a>TBD
							</p>

						</header>
						<p class="rmargin" align="justify">
							Massively parallel (GPUs and other data-parallel accelerators) devices are delivering more
							and
							more computing powers required by modern society. With the growing popularity of massively
							parallel devices, users demand better performance, programmability, reliability, and
							security.
							The goal of this workshop is to provide a forum to discuss massively parallel applications,
							environments, platforms, and architectures, as well as infrastructures that facilitate
							related
							research. This year, we are no longer limited to GPU applications and architectures. We
							welcome research related to any highly parallel computing accelerators and devices.
							Authors are invited to submit papers of original research in the general area of massively
							parallel computing and architectures. Topics include, but are not limited to:
						</p>
						<ul class="">
							<li>Security for GPU architecture and other
								accelerators</li>
							<li>AR/VR support using GPUs or other accelerators
							</li>
							<li>Heterogeneous systems</li>
							<li>Cloud-based GPU computing</li>
							<li>Serverless/disaggregated GPU computing</li>
							<li>GPU/accelerator virtualization/containerization
							</li>
							<li>GPU applications</li>
							<li>GPU performance evaluation/benchmarking</li>
							<li>GPU programming languages</li>
							<li>Operating system support for GPU execution</li>
							<li>GPU compilation techniques </li>
							<li>GPU reliability</li>
							<li>GPU hardware architecture for graphics and
								general-purpose applications</li>
							<li>Power-constrained GPU techniques</li>
							<li>Multi-GPU systems</li>
							<li>Network system design for intra- and
								inter-accelerator communication</li>
							<li>Domain-specific accelerators</li>
							<li>Research & design tools for GPU development
							</li>

						</ul>
					</div>
					<!-- <span class="image object"> banner image
						<img src="images/pic10.jpg" alt="" />
					</span> -->
				</section>

				<section>
					<header class="major" id="program">
						<h2>Workshop Program</h2>
					</header>
					<div>
						<!-- <h3>All times are in US Eastern time (UTC-4). </h3> -->
						<table class="tg" style="table-layout: fixed; width: 100%">
							<colgroup>
								<col style="width: 160px">
								<col style="width: 730px">
							</colgroup>
							<!-- <tr style="border-bottom: 1px solid #000;border-top: 1px solid #000;">
								<td class="tg-fymr">07:00 AM - 09:00 AM</td>
								<td class="tg-0pky">Breakfast</td>
							  </tr> -->
							<tr style="border-bottom: 1px solid #000;border-top: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">9:00 AM - 9:10 AM</span></td>
								<td class="tg-0pky">
									Opening Remarks
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-0pky"><span style="font-weight:bold"> 9:10 AM - 10:00 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Keynote I]  </span>
									<span style="font-style:italic"> Challenges and Opportunities in Providing Trusted Execution Environment on GPUs </span> <br>Yan Solihin, UCF 
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												As GPUs become more and more popular in cloud computing, it is important to address one feature that they lack compared to CPUs: secure execution environment. Currently, users of cloud computing must choose between running their program in a secure environment on CPUs, sacrificing performance and parallelism, or running it on GPUs, sacrificing security. In this talk I will discuss some of the challenges in providing Trusted Execution Environment (TEE) on GPUs.
								First, I will discuss two secure memory architectures, counter-mode encryption and direct encryption, for GPUs and show that we need to architect secure memory differently from it for CPUs. As GPUs are designed for high-throughput computation, its secure memory needs to deliver high bandwidth. Furthermore, with counter-mode encryption, the memory traffic resulting from the metadata, i.e., the counters, MACs (message-authentication codes), and integrity tree, may cause significant performance degradation, even in the presence of metadata caches. Moreover, the sectored cache structure adopted by GPUs leads to multiple sequential accesses to the same metadata cache line, which necessitates the use of MSHRs (miss-status handling registers) for meta-data caches. Finally, unlike CPUs, separate/partitioned metadata caches perform better than unified metadata caches on GPUs. I will discuss some possible solutions to the above problem.
								Besides of providing TEEs on GPUs, we must also consider how CPU and GPU TEEs interact. Without co-designing them, we impose a separate memory encryption domain from the host TEE, causing a very substantial slowdown for communicating data from/to the host. I will discuss a flexible GPU memory encryption design that relies on software memory encryption aided by small architecture support. LITE's flexibility allows GPU TEE to be co-designed with CPU to create a unified encryption domain. We show that GPU applications can be adapted to the use of LITE encryption APIs without major changes.
											</p>
										</div>
									</div>

									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/-Kpl2jZQOis"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-0pky"><span style="font-weight:bold">10:00 AM - 10:30 AM</span></td>
								<td class="tg-0pky">Break</td>
							</tr>
							<!-- <tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr" style="font-weight:old">
									Session 1</td>
								<td>Session Chair: Daniel Wong</td>
							</tr> -->
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">10:30 AM - 10:50 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic"> GPU Auto-tuning Framework for Optimal Performance and Power Consumption</span><br>Sunbal Cheema and Gul Khan (Toronto Metropolitan (formerly Ryerson) University)
									<!-- <span style="font-style:italic">Understanding Portability of Automotive Workload: A Case Study with a Points-to-image Kernel in SYCL on Heterogeneous Computing Platforms</span><br> Zheming Jin and Jeffrey Vetter (ORNL)
									<br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p> An auto-tuning framework for GPU devices is presented for tuning OpenCL application kernels. The GPU tuner employs multi-objective optimization methodology to improve the performance and power consumption of applications. The proposed approach explores a user defined solution space efficiently comprising of possible tunable algorithmic with the hardware counter variations through code transformations. The methodology targets GPU code tuning situations where performance and energy consumption are critical and an optimal trade-off between performance and energy is unavoidable. The proposed framework, MOKAT is evaluated for 2D Convolution kernel targeting NVIDIA Tesla GPU. Various hardware counter and algorithmic variations such as loop unrolling, caching, workgroup size and memory utilization are employed for evaluation. MOKAT utilizes a non-dominated Genetic Algorithm (GA) with hardware power sensor data for application code transformation through code rewrite and validation. The proposed technique is applicable to both off-line and in-loop tuning of kernels. The final Pareto optimal configuration code utilized around 30% less power and 4% faster execution time. The analysis on the first and last GA generation shows the convergence of optimization, and standard deviation is improved by 45%.
											</p>
										</div>
									</div>

									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/tuBSmNzfXQ0"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
				
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">10:50 AM - 11:10 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic">  LATOA: Load-Aware Task Offloading and Adoption in GPU</span><br>Hossein Bitalebi (KTH Royal Institute of Technology), Vahid Geraeinejad (KTH Royal Institute of Technology), Farshad Safaei (Shahid Beheshti University) and Masoumeh Ebrahimi (KTH Royal Institute of Technology)<br>
									<!-- <span style="font-style:italic"> GPU Auto-tuning Framework for Optimal Performance and Power Consumption</span><br>Sunbal Cheema and Gul Khan (Toronto Metropolitan (formerly Ryerson) University) -->
									<!-- <br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												The emerging new applications, such as data mining and graph analysis, demand extra processing power at the hardware level. Conventional static task scheduling is no longer able to meet the requirements of such complicated applications. This inefficiency is a major concern when the application is supposed to run on a Graph-
ics Processing Unit (GPU), where millions of instructions should be distributed among a limited number of processing cores. A non-optimal scheduling strategy leads to unfair load distribution among the GPU’s processing cores. Consequently, while busy cores are stalled due to the lack of resources, waiting for their data from the main memory, other cores are idle, waiting for busy cores to complete their tasks. This leads to a significant number of stall and idle cycles, limiting the GPU performance to a large extent. Our study introduces LATOA, a Load-Aware Task Offloading and Adoption method that tackles this problem by reducing both stall and idle cycles. LATOA is the first study moving from static to dynamic task scheduling based on run-time information obtained from the Miss Status Holding Register (MSHR) tables. In LATOA, all processing cores are dynamically tagged with critical, neutral,
or relaxed states. Then, irregular warps with low locality properties are detected and offloaded from critical cores (going to the stall state) to relaxed ones (going to the idle state). Based on our experiments, LATOA reduces the number of stall cycles on average by 24% and increases the neutral states on average by 38%. In addi-
tion, with negligible hardware overhead, LATOA improves system performance and power efficiency on average by 26% and 7%, respectively.
											</p>
										</div>

									</div>

									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/okPY5dck-qk"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>	

								</td>
							</tr>

								
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">11:10 PM - 12:00 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Invited Talk]</span>
									<span style="font-style:italic"> AMD Instinct ROCm Software Ecosystem and HIP Programming Support</span>
									<br>Timour Paltashev and Trinayan Baruah, AMD </br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												
											</p>
										</div>
									</div>


									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/roAf0-pQe8E"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>	

								</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">12:00 PM - 1:20 PM</span></td>
								<td class="tg-0pky">Lunch</td>
							</tr>
			
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">1:20 PM - 2:20 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Keynote II] </span>
									<span style="font-style:italic"> On-Chip GPU Bandwidth Confusion </span> <br>John Kim, KAIST
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												It is well-known that memory bandwidth is a critical resource in modern GPUs and can be the bottleneck for many workloads. However, high on-chip communication bandwidth is also necessary to ensure that off-chip bandwidth is not bottlenecked by on-chip bandwidth. In this talk, I will explore the challenges of bandwidth challenges internal to the GPU including the on-chip interconnect. In particular, I will re-visit the design of GPU NoC and how bandwidth hierarchy needs to be properly considered to avoid bandwidth bottlenecks in a GPU.  The two important characteristics of on-chip communication are bandwidth and latency; I will describe the unique on-chip bandwidth and latency characteristics and their implication on overall performance. In addition, I will describe how high-bandwidth on-chip bandwidth can be exploited for high-bandwidth side-channel attacks.
											</p>
										</div>
									</div>


									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/pZypA1R_mUo"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>	
								</td>
							</tr>	

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">2:20 PM - 2:40 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper] </span>
									<span style="font-style:italic">Understanding Portability of Automotive Workload: A Case Study with a Points-to-image Kernel in SYCL on Heterogeneous Computing Platforms</span><br> Zheming Jin and Jeffrey Vetter (ORNL)
									<br>
									<!-- <span style="font-style:italic"> On-Chip GPU Bandwidth Confusion </span> <br>John Kim, KAIST
									<br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												SYCL is a promising programming model for heterogenous computing
across vendors’ devices. In this paper, we study whether
SYCL can be applied to an automotive workload and its portability
on heterogeneous computing platforms. We explain the automotive
benchmark, describe our implementations and optimizations of the
benchmark, and evaluate the performance of the benchmarks using
SYCL and other programming models on heterogeneous devices.
The study also allows us to have a better understanding of portability
of the benchmark across these platforms.
											</p>
										</div>
									</div>

									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/n95rYh-xeVA"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>	


								</td>
							</tr>



							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">2:40 AM - 3:00 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic">   Simple Out of Order Core for GPGPUs</span><br>Rodrigo Huerta (Polytechnic University of Catalonia), Jose-Maria Arnau (Semidynamics) and Antonio González (Polytechnic University of Catalonia)
<br>
									<!-- <br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												GPU architectures have become popular for executing general-purpose programs which rely on having a large number of threads that run concurrently to hide the latency among dependent instructions. This approach has an important cost/overhead in terms of low data locality due to the increased pressure on the memory hierarchy of the many threads being run concurrently and the extra cost of storing and managing the on-chip state of those many threads.
This paper presents SOCGPU (Simple Out-of-order Core for GPU), a simple out-of-order execution mechanism that does not require register renaming nor scoreboards. It uses a small Instruction Buffer and a tiny Dependence matrix to keep track of dependencies among instructions and avoid data hazards. Evaluations for an Nvidia Tesla V100-like GPU show that SOCGPU provides a speed-up of up to 2.3 in some machine learning programs and 1.38 on average for a variety of benchmarks, while it reduces energy consumption by 6.5%, with only 2.4% area overhead.
											</p>
										</div>
									</div>

									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/v_UoodJ5B3g"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
									
								</td>
							</tr>
			
							
							<!-- <tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr" style="font-weight:bold">
									Session 2</td>
								<td>Session Chair: Hoda NaghibiJouybari</td>
							</tr> -->

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">3:00 PM - 3:20 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic">   Lightweight Register File Caching in Collector Units for GPUs </span>
									<br>Mojtaba Abaie Shoushtary, Jose Maria Arnau, Jordi Tubella Murgadas and Antonio Gonzalez (Polytechnic University of Catalonia)</br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												Modern GPUs benefit from a sizable Register File (RF) to provide
fine-grained thread switching. As the RF is huge and accessed
frequently, it consumes a considerable share of the dynamic energy
of the GPU. Designing a large, high-throughput RF with low energy
consumption and area for GPUs is challenging.
In this paper, an energy-efficient hierarchical RF design for GPUs,
called Malekeh, is introduced. Malekeh keeps registers in energy-
efficient small caches and maximizes cache efficacy by using light-
weight policies and supporting adaptive algorithms. The policies’
effectiveness is improved by leveraging register reuse distance in-
formation provided by the compiler as a hint. Malekeh reduces the
RF reads by 48.5% and dynamic energy by 29.1%. It also improves
performance by 9.6% with a negligible overhead of 0.04% in area.
											</p>
										</div>
									</div>
								</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">3:20 PM - 3:40 PM</span></td>
								<td class="tg-0pky">Break</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">3:40 PM - 3:55 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Short Paper]</span>
									<span style="font-style:italic">  Exploiting Scratchpad Memory for Deep Temporal Blocking</span>
									<br>Lingqi Zhang (Tokyo Institute of Technology), Mohamed Wahib (RIKEN Center for Computational Science), Peng Chen (National Institute of Advanced Industrial Science and Technology), Jintao Meng (Shenzhen Institutes of Advanced Technology), Xiao Wang (Oak Ridge National Laboratory), Endo Toshio (Tokyo Institute of Technology) and Satoshi Matsuoka (RIKEN Center for Computational Science) </br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												General Purpose Graphics Processing Units (GPGPU) power most of the top systems in HPC. The total capacity of scratchpad memory has increased by more than $20$x in the last decade. However, existing optimizations for stencil computations using temporal blocking have yet to aggressively exploit the large capacity of scratchpad memory. This work uses the 2D Jacobian 5-point iterative stencil as a case for studying the utilization of the large scratchpad memory. Unlike the existing research that tile the domain in a thread block fashion, we tile the domain so that every single tile is large enough to employ all the available scratchpad memory in the GPU. Hence, we process several time steps inside a single tile before offloading the result back to global memory. The evaluated result shows our performance is comparable to state-of-the-art implementations, yet our implementation is much simpler, and does not need auto-generation of code.
											</p>
										</div>
									</div>


									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/nteq6DtSD4Y"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>


								</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">3:55 PM - 4:10 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Short Paper]</span>
									<span style="font-style:italic">   Understanding Scalability of Multi-GPU Systems</span>
									<br>Yuan Feng and Hyeran Jeon (University of California, Merced)
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>
												Multi-GPU systems are considered as one of the most promising scalable accelerator systems. There have been several studies that tackled communication and scheduling efficiencies with a small-scale multi-GPU system (mostly with four GPU modules). In this paper, we examine scalability by increasing the number of GPUs. Our observations show that multi-GPU systems are yet to be scalable, mainly due to Non-Uniform Memory Access (NUMA) effects; furthermore, the state-of-the-art aggressive page distribution is one of the main reasons that increase slow remote accesses.
											</p>
										</div>
									</div>
									
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/Bw3YRiloLVM"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
									

								</td>
							</tr>

							




							
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">4:10 PM - 4:20 PM</span></td>
								<td class="tg-0pky">
									Closing Remarks
								</td>
							</tr>
						</table>
					</div>
				</section>


				<!-- Section -->
				

					<section>
						<header class="major" id="importantdates">
							<h2>Important Dates</h2>
						</header>

						<ul>
							<li>
								Papers due:
								<!-- <s>Nov 28, 2022</s> -->
								<s>Novemeber 28, 2022</s>
								December 9, 2022
							</li>
							<li>Notification: Jan 6, 2023 </li>
							<li>Final paper due: Feb 17, 2023 </li>
						</ul>


					</section>

					<section>
						<header class="major" id="guidelines">
							<h2>Submission Guidelines</h2>
						</header>
						<p>
							Full paper submissions must be in PDF format for US letter-size paper. They must not exceed <b> 6
								pages (all-inclusive)</b> in standard ACM two-column conference format (review mode, with page
numbers and both 9 or 10pt can be used). Publication in GPGPU does not preclude publication
of longer submissions of the work to subsequent conferences or journals. GPGPU also accepts
<b>extended abstracts (2 pages including references)</b> on work in progress of relevant topics.
Authors can select if they want to reveal their identity in the submission.
							Templates for ACM format are available for Microsoft Word, and LaTeX at: <a
								href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>
						</p>

						<!-- <p>
							At least one author must present at the workshop
							conference. Travel func may be applied through
							SIGPLAN
							Professional Activities Committee (PAC). Details
							are
							available <a href="https://www.sigplan.org/PAC/">here</a>
						</p> -->

						<p>
							Submission Site: <a href="https://easychair.org/conferences/?conf=gpgpu2023">GPGPU
								2023</a>
						</p>


					</section>

					<section>
						<header class="major" id="organizers">
							<h2>Workshop Organizers</h2>
						</header>

						<div class="content">
							<table style="width:100%">
								<col style="width:20%">
								<col style="width:20%">
								<col style="width:20%">
								<tr>
									<td><img src="images/portraits/hyeran.jpg" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' /></td>
									<td><img src="images/portraits/yifan.jpg" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' />
									</td>
									<td><img src="images/portraits/daniel.png" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' /></td>
								</tr>
								<tr>
									<td><a href="https://www.mocalab.org/hyeran-jeon"
											target=&ldquo;blank&rdquo;>Hyeran Jeon</a></td>
									<td><a href="https://syifan.github.io/"
											target=&ldquo;blank&rdquo;>Yifan
											Sun</a></td>
									<td><a href="https://www.danielwong.org/"
											target=&ldquo;blank&rdquo;>Daniel
											Wong</a>
									</td>
								</tr>
								<tr>
									<td>Co-chair</td>
									<td>Co-chair</td>
									<td>Co-chair</td>
								</tr>
								<tr>
									<td>UC Merced</td>
									<td>William &amp; Mary</td>
									<td>UC Riverside</td>
								</tr>
								<tr>
									<td>Hyeran Jeon is an Assistant Professor in the Department of Computer Science and Engineering
at the University of California, Merced. She received her PhD at the University of Southern
California. Her research interests lie in energy-efficient, reliable, and secure GPU architectures.</td>
									<td>
										Yifan Sun is an Assistant Professor in the Department of Computer Science at William & Mary
since Fall 2020. He received his Ph.D. degree from the Department of Electrical and Computer
Engineering at Northeastern University in 2020. His research interests lie in GPU architecture,
performance evaluation, and performance modeling.
									</td>
									<td>
										Daniel Wong is an Associate Professor in the Department of Electrical and Computer
Engineering at the University of California, Riverside. He received his PhD in Electrical
Engineering at the University of Southern California (USC). His research spans GPU
Architecture, High Performance Computing, and Warehouse-scale Computing. His current
research focuses on energy efficient and high performance computing systems from datacenter
scale to micro-architectures. His research work has been recognized with an IEEE MICRO Top
Picks in 2012 and an NSF CAREER award in 2020.
									</td>
								</tr>
							</table>

						<div class="content">
							<table style="width:100%">
								<col style="width:20%">
								<col style="width:20%">
								<tr>
									<td><img src="images/portraits/nafis.jpeg" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' /></td>
									<td><img src="images/portraits/yuan-headshot.jpg" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' />
									</td>
								<tr>
									<td><a href="index.html"
											target=&ldquo;blank&rdquo;>Nafis Mustakin</a></td>
									<td><a href="https://yfeng-44.github.io/"
											target=&ldquo;blank&rdquo;>Yuan
											Feng</a></td>
								</tr>
								<tr>
									<td>Publication Chair</td>
									<td>Web Chair</td>
								</tr>
								<tr>
									<td>UC Riverside</td>
									<td>UC Merced</td>
								</tr>
							</table>
							Please contact the organizers if you have any
							questions.
						</div>

						
					</section>

					<section>
						<header class="major" id="pcmember">
							<h2>Program Committee</h2>
						</header>
						<ul>
							<li>Zhongliang Chen (AMD)
							</li>
							<li>Xulong Tang (U Pitts)
							</li>
							<li>Wenqian Dong (Florida International University)</li>
							<li>José L. Abellán (UCAM)</li>
							<li>Gunjae Koo (Korea University)</li>
							<li>Hoda Naghibijouybari (Binghamton University)</li>
							<li>Adwait Jog (William & Mary)</li>
							<li>David Kaeli (Northeastern University)
							</li>
							<li>Shi Dong (Cerebras)</li>
						</ul>
					</section>

					<section>
						<header class="major" id="proceedings">
							<h2>Proceedings</h2>
						</header>
							<div class="content">
The accepted papers will be published in the ACM Online Conference Proceedings Series.
							</div>
					</section>

					
				

				<section id="keynote">
					<header id="header">
						<h2>Keynote Speakers</h2>
						<!-- <a class="logo"><strong>
								<font size=6em>Keynotes</font>
							</strong></a> -->
					</header>
					<!-- </section> -->
					<section id="banner">
						<div class="content">
							<div style="float: left; margin-right: 10px;"><img
									src="images/portraits/ucf_99866399.jpg" height="360" /> </div>
							<b>Speaker:</b> Yan Solihin<br />
							<b>Title: Challenges and Opportunities in Providing Trusted Execution Environment on GPUs </b><br /><br />
							<p align="justify"><b>Abstract:</b>As GPUs become more and more popular in cloud computing, it is important to address one feature that they lack compared to CPUs: secure execution environment. Currently, users of cloud computing must choose between running their program in a secure environment on CPUs, sacrificing performance and parallelism, or running it on GPUs, sacrificing security. In this talk I will discuss some of the challenges in providing Trusted Execution Environment (TEE) on GPUs.
								First, I will discuss two secure memory architectures, counter-mode encryption and direct encryption, for GPUs and show that we need to architect secure memory differently from it for CPUs. As GPUs are designed for high-throughput computation, its secure memory needs to deliver high bandwidth. Furthermore, with counter-mode encryption, the memory traffic resulting from the metadata, i.e., the counters, MACs (message-authentication codes), and integrity tree, may cause significant performance degradation, even in the presence of metadata caches. Moreover, the sectored cache structure adopted by GPUs leads to multiple sequential accesses to the same metadata cache line, which necessitates the use of MSHRs (miss-status handling registers) for meta-data caches. Finally, unlike CPUs, separate/partitioned metadata caches perform better than unified metadata caches on GPUs. I will discuss some possible solutions to the above problem.
								Besides of providing TEEs on GPUs, we must also consider how CPU and GPU TEEs interact. Without co-designing them, we impose a separate memory encryption domain from the host TEE, causing a very substantial slowdown for communicating data from/to the host. I will discuss a flexible GPU memory encryption design that relies on software memory encryption aided by small architecture support. LITE's flexibility allows GPU TEE to be co-designed with CPU to create a unified encryption domain. We show that GPU applications can be adapted to the use of LITE encryption APIs without major changes. </p>
							<p align="justify"><b>Bio:</b> Yan Solihin is Charles N. Millican Chair Professor of Computer Science at University of Central Florida. He is the Director of UCF Cybersecurity and Privacy Cluster, leading a team of 13 faculty members working in various areas of cyber security. Prior to joining UCF, from 2002-2018, he was a Professor of Electrical and Computer Engineering at NCSU. From 2015-2018, he was a Program Director at the Division of Computer and Network Systems (CNS) at the National Science Foundation. His responsibilities include managing the Secure and Trustworthy Cyberspace (SaTC), Computer Systems Research (CSR), and Scalability and Parallelism in the eXtreme (SPX). He co-founded the NSF/Intel Partnership on Foundational Microarchitecture Research (FoMR) program.
 
								He obtained his Ph.D. from the University of Illinois at Urbana-Champaign (UIUC) in 2002. He is a recipient of 2010 and 2005 IBM Faculty Partnership Award, 2004 NSF Faculty Early Career Award, and 1997 AT&T Leadership Award. He is well known for pioneering cache sharing fairness and Quality of Service (QoS), efficient counter mode memory encryption, and Bonsai Merkle Tree, which have significantly influenced Intel Cache Allocation Technology and Secure Guard eXtension (SGX)'s Memory Encryption Engine (MEE). In recognition, he received IEEE Fellow “for contributions to shared cache hierarchies and secure processors” in 2017. He is listed in the HPCA Hall of Fame, ISCA Hall of Fame, and Computer Architecture Total (CAT) Hall of Fame. He has published 100+ papers, authored 120+ patent assets, delivered 80+ invited talks/seminars, including several keynotes and multi-day tutorials. His research received HPCA Test of Time Award (2023), MICRO Best Paper Runner-up Award (2017), IEEE Micro Top Picks (2011), and several Best Paper nominations/finalists (ISPASS 2013, IPDPS 2012, and HPCA 2005).</p>
							<br />

						</div>
					</section>

					<section id="banner">
						<div class="content">
							<div style="float: left; margin-right: 10px;"><img
									src="images/portraits/JohnKim-1.jpg" height="360" /> </div>
							<b>Speaker:</b> John Kim<br />
							<b>Title: On-Chip GPU Bandwidth Confusion </b><br /><br />
							<p align="justify"><b>Abstract:</b> It is well-known that memory bandwidth is a critical resource in modern GPUs and can be the bottleneck for many workloads. However, high on-chip communication bandwidth is also necessary to ensure that off-chip bandwidth is not bottlenecked by on-chip bandwidth. In this talk, I will explore the challenges of bandwidth challenges internal to the GPU including the on-chip interconnect. In particular, I will re-visit the design of GPU NoC and how bandwidth hierarchy needs to be properly considered to avoid bandwidth bottlenecks in a GPU.  The two important characteristics of on-chip communication are bandwidth and latency; I will describe the unique on-chip bandwidth and latency characteristics and their implication on overall performance. In addition, I will describe how high-bandwidth on-chip bandwidth can be exploited for high-bandwidth side-channel attacks. </p>
							<p align="justify"><b>Bio:</b>John Kim is currently a full professor in the School of Electrical Engineering at KAIST (Korea Advanced Institute of Science and Technology) in Daejeon, Korea. John Kim received his Ph.D. from Stanford University and B.S/M.Eng from Cornell University. His research interests include computer architecture, interconnection networks, security, and mobile systems.  He has received a Google Faculty Research Award, Microsoft-Asia New Faculty Fellowship, and is listed in the Hall of Fame for ISCA, MICRO, and HPCA. He has also worked on the design of several microprocessors at Intel and at Motorola.</p> 
							<br />
						</div>
					</section>

					<section>
						<header class="major" id="publicity">
							<h2>Publicity</h2>
						</header>
							<div class="content">
								TBD
							</div>
					</section>

					<section>
						<header class="major" id="hist">
							<h2>Workshop Attendance and Impact</h2>
						</header>

						<div class="content">
							<div>
								In general, GPGPU has been one of the highest attended workshops at PPoPP or ASPLOS. Generally, 50-75 people register for the workshop. The average citation count (as per Google Scholar), for a GPGPU workshop paper, is currently ~37.5, where there have been 8 influential papers with 100+ citations.
							</div>
							<br />
						<h2>About Prior GPGPU Workshop Meetings</h2>
						<div>
							David Kaeli (Northeastern) and John Cavazos (Delaware) started this GPGPU workshop series,
which was first held in 2007 at Northeastern University. In 2008, the workshop was held with
ASPLOS 2008. This trend continued and this GPGPU workshop was held with ASPLOS for the
next 6 years. From 2015 to 2018, the GPGPU workshop was co-located with PPoPP. GPGPU
2019 workshop was held with ASPLOS 2019. The last two GPGPU workshops (2020, 2022)
was again co-located with PPoPP.
						</div>
							<h2>Previous versions of the GPGPU workshop:</h2>
							<ul>
								<li><a href="https://sarchlab.github.io/gpgpu2022/" target=&ldquo;blank&rdquo;>GPGPU
										14 (2022)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2022.html"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://insight-archlab.github.io/gpgpu.html"
										target=&ldquo;blank&rdquo;>GPGPU
										13 (2020)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2020.html"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://insight-archlab.github.io/gpgpu12.html"
										target=&ldquo;blank&rdquo;>GPGPU 12
										(2019)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2019.html"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://gpgpu11.000webhostapp.com/" target=&ldquo;blank&rdquo;>GPGPU 11
										(2018)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2018"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://gpgpu10.athoura.com/" target=&ldquo;blank&rdquo;>GPGPU 10
										(2017)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2017"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://conf.researchr.org/track/PPoPP-2016/GPGPU-2016-papers"
										target=&ldquo;blank&rdquo;>GPGPU 09
										(2016)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2016"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU8/"
										target=&ldquo;blank&rdquo;>GPGPU 08
										(2015)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2015"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU6/"
										target=&ldquo;blank&rdquo;>GPGPU 07
										(2014)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2014"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU6/"
										target=&ldquo;blank&rdquo;>GPGPU 06
										(2013)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2013"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2012" target=&ldquo;blank&rdquo;>GPGPU
										05
										(2012)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2012"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2011" target=&ldquo;blank&rdquo;>GPGPU
										04
										(2011)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2011"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2010" target=&ldquo;blank&rdquo;>GPGPU
										03
										(2010)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2010"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2009" target=&ldquo;blank&rdquo;>GPGPU
										02
										(2009)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2009"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li>GPGPU 01 (2008)
								</li>
							</ul>
						</div>
					</section>



			</div>
		</div>

		<!-- Sidebar -->
		<div id="sidebar">
			<div class="inner">

				<!-- Menu -->
				<nav id="menu">
					<!-- <header class="major">
						<h2>MENU</h2>
					</header> -->
					<ul>
						<li><a href="index.html">Home</a></li>
						<!-- <li><a href="#program">Program</a></li> -->
						<!-- <li><a href="#keynote">Keynotes</a></li> -->
						<li><a href="#importantdates">Important Dates</a></li>
						<li><a href="#guidelines">Submission Guidelines</a>
						</li>
						<li><a href="#organizers">Workshop Organizers</a></li>
						<li><a href="#pcmember">Program Committee</a></li>
						<li><a href="#proceedings">Proceedings</a></li>
						<li><a href="#hist">History and Impact</a></li>

					</ul>
				</nav>
			</div>
		</div>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>