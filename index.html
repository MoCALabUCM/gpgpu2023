<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>GPGPU 2023</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<div id="main">
			<div class="inner">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo"><strong>GPGPU</strong>
						2023</a>
					<!-- <ul class="icons">
						<li><a href="#" class="icon brands fa-twitter"><span
									class="label">Twitter</span></a></li>
						<li><a href="#" class="icon brands fa-facebook-f"><span
									class="label">Facebook</span></a></li>
						<li><a href="#"
								class="icon brands fa-snapchat-ghost"><span
									class="label">Snapchat</span></a></li>
						<li><a href="#" class="icon brands fa-instagram"><span
									class="label">Instagram</span></a></li>
						<li><a href="#" class="icon brands fa-medium-m"><span
									class="label">Medium</span></a></li>
					</ul> -->
				</header>

				<!-- Banner -->
				<section id="banner">
					<div class="content">
						<header>
							<h1>GPGPU 2023</h1>
							<p>The 15th Workshop on General Purpose Processing
								Using GPU (GPGPU 2023)</p>
							<!-- place and date -->
							<!-- <p>April 3, 2022, Online</p>   -->
							<p>Feb 25 Full Day, 2023, Montreal, Canada</p>

							<!-- <p>Zoom Link: <a
									href="https://acm-org.zoom.us/j/96884634475?pwd=d1hCeFc1VFhOMnNjZnk4Z05UQ0ZwZz09">https://acm-org.zoom.us/j/96884634475?pwd=d1hCeFc1VFhOMnNjZnk4Z05UQ0ZwZz09</a>
							</p> -->
							<!-- <p>Zoom Link: <a href="TBD"></a>TBD
							</p> -->

						</header>
						<p class="rmargin" align="justify">
							Massively parallel (GPUs and other data-parallel accelerators) devices are delivering more
							and
							more computing powers required by modern society. With the growing popularity of massively
							parallel devices, users demand better performance, programmability, reliability, and
							security.
							The goal of this workshop is to provide a forum to discuss massively parallel applications,
							environments, platforms, and architectures, as well as infrastructures that facilitate
							related
							research. This year, we are no longer limited to GPU applications and architectures. We
							welcome research related to any highly parallel computing accelerators and devices.
							Authors are invited to submit papers of original research in the general area of massively
							parallel computing and architectures. Topics include, but are not limited to:
						</p>
						<ul class="">
							<li>Security for GPU architecture and other
								accelerators</li>
							<li>AR/VR support using GPUs or other accelerators
							</li>
							<li>Heterogeneous systems</li>
							<li>Cloud-based GPU computing</li>
							<li>Serverless/disaggregated GPU computing</li>
							<li>GPU/accelerator virtualization/containerization
							</li>
							<li>GPU applications</li>
							<li>GPU performance evaluation/benchmarking</li>
							<li>GPU programming languages</li>
							<li>Operating system support for GPU execution</li>
							<li>GPU compilation techniques </li>
							<li>GPU reliability</li>
							<li>GPU hardware architecture for graphics and
								general-purpose applications</li>
							<li>Power-constrained GPU techniques</li>
							<li>Multi-GPU systems</li>
							<li>Network system design for intra- and
								inter-accelerator communication</li>
							<li>Domain-specific accelerators</li>
							<li>Research & design tools for GPU development
							</li>

						</ul>
					</div>
					<!-- <span class="image object"> banner image
						<img src="images/pic10.jpg" alt="" />
					</span> -->
				</section>

				<section>
					<header class="major" id="program">
						<h2>Workshop Program</h2>
					</header>
					<div>
						<!-- <h3>All times are in US Eastern time (UTC-4). </h3> -->
						<table class="tg" style="table-layout: fixed; width: 100%">
							<colgroup>
								<col style="width: 160px">
								<col style="width: 730px">
							</colgroup>
							<!-- <tr style="border-bottom: 1px solid #000;border-top: 1px solid #000;">
								<td class="tg-fymr">07:00 AM - 09:00 AM</td>
								<td class="tg-0pky">Breakfast</td>
							  </tr> -->
							<tr style="border-bottom: 1px solid #000;border-top: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">9:00 AM - 9:10 AM</span></td>
								<td class="tg-0pky">
									Opening Remarks
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-0pky"><span style="font-weight:bold"> 9:10 AM - 10:00 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Keynote I]</span>
									<!-- <span style="font-style:italic">Building
										Performant and Portable Heterogenous
										Code using GPU Compute
										Accelerators</span> <br>Derek Bouius,
									AMD
									<a href="#keynote">[Link]</a>
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>This talk will describe the
												various
												methodologies used to offload
												computationally intensive
												workloads
												from CPUs to accelerators. Key
												topics will cover HW
												architecture
												considerations and programming
												methodologies along with
												debugging
												and profiling techniques.</p>
										</div>
									</div> -->
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-0pky"><span style="font-weight:bold">10:00 AM - 10:30 AM</span></td>
								<td class="tg-0pky">Break</td>
							</tr>
							<!-- <tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr" style="font-weight:bold">
									Session 1</td>
								<td>Session Chair: Daniel Wong</td>
							</tr> -->
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">10:30 AM - 10:50 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic">Understanding Portability of Automotive Workload: A Case Study with a Points-to-image Kernel in SYCL on Heterogeneous Computing Platforms</span><br> Zheming Jin and Jeffrey Vetter (ORNL)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<!-- <p>Emerging advanced applications,
												such as deep learning and
												graphprocessing, with enormous
												processing demand and massive
												mem-ory requests call for a
												comprehensive processing system
												or ad-vanced solutions to
												address these requirements.
												Near data process-ing is one of
												the promising structures
												targeting this goal.
												However,most recent studies
												have focused on processing
												instructions nearthe main
												memory data banks while
												ignoring the benefits of
												pro-cessing instructions near
												other memory hierarchy levels
												such as LLC. In this study, we
												investigate the near LLC
												processing struc-tures, and
												compare it to the near main
												memory processing alter-native,
												specifically in graphical
												processing units. We analyze
												thesetwo structures on various
												applications in terms of
												performance andpower. Results
												show a clear benefit of near
												LLC processing overnear main
												memory processing in a class of
												applications. Further,we
												suggest a structure, which
												could benefit from both
												structures,requiring the
												applications to be
												characterized in advance or at
												runtime.</p> -->
										</div>
									</div>
				
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">10:50 AM - 11:10 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic"> GPU Auto-tuning Framework for Optimal Performance and Power Consumption</span><br>Sunbal Cheema and Gul Khan (Toronto Metropolitan (formerly Ryerson) University)
<br>
									<!-- <br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<!-- <p>When running single-GPU
												applications on multi-GPU
												compute nodes, the remaining
												GPU devices are kept idle. We
												propose a novel technology to
												accelerate these single-GPU
												applications using the idle GPU
												devices. The data transfers
												between host and device are
												performed not only by the first
												GPU but also by the second GPU
												as well as the alternative
												route with PCI-Express and
												NV-Link connected to it. Our
												performance evaluations show
												the proposed method enables
												about twice data transfer speed
												as native single GPU case for
												large data sizes.</p> -->
										</div>
									</div>
								</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">11:10 AM - 11:30 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic">  LATOA: Load-Aware Task Offloading and Adoption in GPU</span><br>Hossein Bitalebi (KTH Royal Institute of Technology), Vahid Geraeinejad (KTH Royal Institute of Technology), Farshad Safaei (Shahid Beheshti University) and Masoumeh Ebrahimi (KTH Royal Institute of Technology)<br>
									<!-- <br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<!-- <p>When running single-GPU
												applications on multi-GPU
												compute nodes, the remaining
												GPU devices are kept idle. We
												propose a novel technology to
												accelerate these single-GPU
												applications using the idle GPU
												devices. The data transfers
												between host and device are
												performed not only by the first
												GPU but also by the second GPU
												as well as the alternative
												route with PCI-Express and
												NV-Link connected to it. Our
												performance evaluations show
												the proposed method enables
												about twice data transfer speed
												as native single GPU case for
												large data sizes.</p> -->
										</div>
									</div>
								</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">11:30 AM - 11:50 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic">   Simple Out of Order Core for GPGPUs</span><br>Rodrigo Huerta (Polytechnic University of Catalonia), Jose-Maria Arnau (Semidynamics) and Antonio González (Polytechnic University of Catalonia)
<br>
									<!-- <br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<!-- <p>When running single-GPU
												applications on multi-GPU
												compute nodes, the remaining
												GPU devices are kept idle. We
												propose a novel technology to
												accelerate these single-GPU
												applications using the idle GPU
												devices. The data transfers
												between host and device are
												performed not only by the first
												GPU but also by the second GPU
												as well as the alternative
												route with PCI-Express and
												NV-Link connected to it. Our
												performance evaluations show
												the proposed method enables
												about twice data transfer speed
												as native single GPU case for
												large data sizes.</p> -->
										</div>
									</div>
								</td>
							</tr>
			
							
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">12:00 PM - 1:30 PM</span></td>
								<td class="tg-0pky">Lunch</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">1:30 PM - 2:20 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Keynote II]</span>
									<!-- <span style="font-style:italic">Re-design
										GPU NoC and LLC
										System</span>
									<br>Xia Zhao (Academy
									of Military Sciences)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>To provide high compute
												power, GPUs feature an
												increased number of SMs
												with a larger LLC size and
												higher memory bandwidth.
												Facing the new
												opportunities and
												challenges, how to design a
												scalable and
												high-performance NoC and
												LLC system becomes
												especially important. In
												this talk, I will introduce
												our recent work including
												hierarchy NoC design for
												GPUs, adaptive memory-side
												last-level GPU caching, and
												selective replication in
												memory-side GPU caches.
												The hierarchy NoC provides
												a scalable and low-cost
												interconnect network to
												connect the SMs with the
												LLCs and memory controllers
												by fully exploiting the
												unique traffic pattern in
												GPUs. To solve the
												contention caused by the
												concurrent memory accesses
												sent to the same shared
												data, adaptive LLC can
												adaptively choose shared
												LLC or private LLC based on
												the application
												characteristics. Compared
												to the coarse grain of data
												replication in adaptive
												LLC, selective replication
												can selectively choose the
												replication degree to
												reduce data contention
												while avoiding the high LLC
												miss rate caused by the
												duplicated data. All these
												designs significantly
												increase GPU performance
												for data-intensive
												applications. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/0Lm9URF90co"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div> -->
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">2:30 PM - 3:00 PM</span></td>
								<td class="tg-0pky">Break</td>
							</tr>

							<!-- <tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr" style="font-weight:bold">
									Session 2</td>
								<td>Session Chair: Hoda NaghibiJouybari</td>
							</tr> -->

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">3:00 PM - 3:20 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Regular Paper]</span>
									<span style="font-style:italic">   Lightweight Register File Caching in Collector Units for GPUs </span>
									<br>Mojtaba Abaie Shoushtary, Jose Maria Arnau, Jordi Tubella Murgadas and Antonio Gonzalez (Polytechnic University of Catalonia)</br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<!-- <p>High-level code generators like
												Halide, Lift, and RISE make a
												compelling proposition: write
												programs in a simple high-level
												language and get
												high-performing GPU code “for
												free”. They achieve this feat
												by restricting the input
												language to a specific domain
												(such as image and array
												processing in Halide) or to a
												fixed set of flexible parallel
												patterns (as Lift and RISE do).
												Implementing high-level code
												generators that produce
												high-performance code is
												challenging, specifically as
												the target hardware constantly
												evolves.
												In this paper, we discuss how
												we systematically extend the
												RISE high-level code generator
												with support for tensor cores,
												a specialized hardware feature
												of recent Nvidia GPUs. We
												highlight the design of RISE
												that makes it easily extensible
												by following a systematic
												bottom-up approach, that first,
												exposes the imperative tensor
												core API to the code generator,
												then, raises the abstractions
												to an internal low-level
												functional representation,
												that, finally, is targeted by a
												rewrite process that starts
												from a high-level functional
												program.
												Our experimental evaluation
												shows that RISE with support
												for tensor cores generates code
												of competitive performance to
												manually optimized CUDA code,
												which is only up to 36%, but on
												average only 10%, slower than
												Nvidia’s highly optimized
												cuBLAS library, and clearly
												outperforms any code that does
												not exploit tensor cores. </p> -->
										</div>
									</div>
								</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">3:20 PM - 3:35 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Short Paper]</span>
									<span style="font-style:italic">  Exploiting Scratchpad Memory for Deep Temporal Blocking</span>
									<br>Lingqi Zhang (Tokyo Institute of Technology), Mohamed Wahib (RIKEN Center for Computational Science), Peng Chen (National Institute of Advanced Industrial Science and Technology), Jintao Meng (Shenzhen Institutes of Advanced Technology), Xiao Wang (Oak Ridge National Laboratory), Endo Toshio (Tokyo Institute of Technology) and Satoshi Matsuoka (RIKEN Center for Computational Science) </br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<!-- <p>High-level code generators like
												Halide, Lift, and RISE make a
												compelling proposition: write
												programs in a simple high-level
												language and get
												high-performing GPU code “for
												free”. They achieve this feat
												by restricting the input
												language to a specific domain
												(such as image and array
												processing in Halide) or to a
												fixed set of flexible parallel
												patterns (as Lift and RISE do).
												Implementing high-level code
												generators that produce
												high-performance code is
												challenging, specifically as
												the target hardware constantly
												evolves.
												In this paper, we discuss how
												we systematically extend the
												RISE high-level code generator
												with support for tensor cores,
												a specialized hardware feature
												of recent Nvidia GPUs. We
												highlight the design of RISE
												that makes it easily extensible
												by following a systematic
												bottom-up approach, that first,
												exposes the imperative tensor
												core API to the code generator,
												then, raises the abstractions
												to an internal low-level
												functional representation,
												that, finally, is targeted by a
												rewrite process that starts
												from a high-level functional
												program.
												Our experimental evaluation
												shows that RISE with support
												for tensor cores generates code
												of competitive performance to
												manually optimized CUDA code,
												which is only up to 36%, but on
												average only 10%, slower than
												Nvidia’s highly optimized
												cuBLAS library, and clearly
												outperforms any code that does
												not exploit tensor cores. </p> -->
										</div>
									</div>
								</td>
							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">3:35 PM - 3:50 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Short Paper]</span>
									<span style="font-style:italic">   Understanding Scalability of Multi-GPU Systems</span>
									<br>Yuan Feng and Hyeran Jeon (University of California, Merced)
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<!-- <p>High-level code generators like
												Halide, Lift, and RISE make a
												compelling proposition: write
												programs in a simple high-level
												language and get
												high-performing GPU code “for
												free”. They achieve this feat
												by restricting the input
												language to a specific domain
												(such as image and array
												processing in Halide) or to a
												fixed set of flexible parallel
												patterns (as Lift and RISE do).
												Implementing high-level code
												generators that produce
												high-performance code is
												challenging, specifically as
												the target hardware constantly
												evolves.
												In this paper, we discuss how
												we systematically extend the
												RISE high-level code generator
												with support for tensor cores,
												a specialized hardware feature
												of recent Nvidia GPUs. We
												highlight the design of RISE
												that makes it easily extensible
												by following a systematic
												bottom-up approach, that first,
												exposes the imperative tensor
												core API to the code generator,
												then, raises the abstractions
												to an internal low-level
												functional representation,
												that, finally, is targeted by a
												rewrite process that starts
												from a high-level functional
												program.
												Our experimental evaluation
												shows that RISE with support
												for tensor cores generates code
												of competitive performance to
												manually optimized CUDA code,
												which is only up to 36%, but on
												average only 10%, slower than
												Nvidia’s highly optimized
												cuBLAS library, and clearly
												outperforms any code that does
												not exploit tensor cores. </p> -->
										</div>
									</div>
								</td>
							</tr>

							

							
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">3:50 PM - 4:00 PM</span></td>
								<td class="tg-0pky">
									Closing Remarks
								</td>
							</tr>
						</table>
					</div>
				</section>


				<!-- Section -->
				

					<section>
						<header class="major" id="importantdates">
							<h2>Important Dates</h2>
						</header>

						<ul>
							<li>
								Papers due:
								<!-- <s>Nov 28, 2022</s> -->
								<s>Novemeber 28, 2022</s>
								December 9, 2022
							</li>
							<li>Notification: Jan 6, 2023 </li>
							<li>Final paper due: Feb 17, 2023 </li>
						</ul>


					</section>

					<section>
						<header class="major" id="guidelines">
							<h2>Submission Guidelines</h2>
						</header>
						<p>
							Full paper submissions must be in PDF format for US letter-size paper. They must not exceed <b> 6
								pages (all-inclusive)</b> in standard ACM two-column conference format (review mode, with page
numbers and both 9 or 10pt can be used). Publication in GPGPU does not preclude publication
of longer submissions of the work to subsequent conferences or journals. GPGPU also accepts
<b>extended abstracts (2 pages including references)</b> on work in progress of relevant topics.
Authors can select if they want to reveal their identity in the submission.
							Templates for ACM format are available for Microsoft Word, and LaTeX at: <a
								href="https://www.acm.org/publications/proceedings-template">https://www.acm.org/publications/proceedings-template</a>
						</p>

						<!-- <p>
							At least one author must present at the workshop
							conference. Travel func may be applied through
							SIGPLAN
							Professional Activities Committee (PAC). Details
							are
							available <a href="https://www.sigplan.org/PAC/">here</a>
						</p> -->

						<p>
							Submission Site: <a href="https://easychair.org/conferences/?conf=gpgpu2023">GPGPU
								2023</a>
						</p>


					</section>

					<section>
						<header class="major" id="organizers">
							<h2>Workshop Organizers</h2>
						</header>

						<div class="content">
							<table style="width:100%">
								<col style="width:20%">
								<col style="width:20%">
								<col style="width:20%">
								<tr>
									<td><img src="images/portraits/hyeran.jpg" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' /></td>
									<td><img src="images/portraits/yifan.jpg" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' />
									</td>
									<td><img src="images/portraits/daniel.png" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' /></td>
								</tr>
								<tr>
									<td><a href="https://www.mocalab.org/hyeran-jeon"
											target=&ldquo;blank&rdquo;>Hyeran Jeon</a></td>
									<td><a href="https://syifan.github.io/"
											target=&ldquo;blank&rdquo;>Yifan
											Sun</a></td>
									<td><a href="https://www.danielwong.org/"
											target=&ldquo;blank&rdquo;>Daniel
											Wong</a>
									</td>
								</tr>
								<tr>
									<td>Co-chair</td>
									<td>Co-chair</td>
									<td>Co-chair</td>
								</tr>
								<tr>
									<td>UC Merced</td>
									<td>William &amp; Mary</td>
									<td>UC Riverside</td>
								</tr>
								<tr>
									<td>Hyeran Jeon is an Assistant Professor in the Department of Computer Science and Engineering
at the University of California, Merced. She received her PhD at the University of Southern
California. Her research interests lie in energy-efficient, reliable, and secure GPU architectures.</td>
									<td>
										Yifan Sun is an Assistant Professor in the Department of Computer Science at William & Mary
since Fall 2020. He received his Ph.D. degree from the Department of Electrical and Computer
Engineering at Northeastern University in 2020. His research interests lie in GPU architecture,
performance evaluation, and performance modeling.
									</td>
									<td>
										Daniel Wong is an Associate Professor in the Department of Electrical and Computer
Engineering at the University of California, Riverside. He received his PhD in Electrical
Engineering at the University of Southern California (USC). His research spans GPU
Architecture, High Performance Computing, and Warehouse-scale Computing. His current
research focuses on energy efficient and high performance computing systems from datacenter
scale to micro-architectures. His research work has been recognized with an IEEE MICRO Top
Picks in 2012 and an NSF CAREER award in 2020.
									</td>
								</tr>
							</table>

						<div class="content">
							<table style="width:100%">
								<col style="width:20%">
								<col style="width:20%">
								<tr>
									<td><img src="images/portraits/nafis.jpeg" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' /></td>
									<td><img src="images/portraits/yuan-headshot.jpg" style="border-radius: 10px;" alt=""
											data-position="center center" width='200' />
									</td>
								<tr>
									<td><a href="index.html"
											target=&ldquo;blank&rdquo;>Nafis Mustakin</a></td>
									<td><a href="https://yfeng-44.github.io/"
											target=&ldquo;blank&rdquo;>Yuan
											Feng</a></td>
								</tr>
								<tr>
									<td>Publication Chair</td>
									<td>Web Chair</td>
								</tr>
								<tr>
									<td>UC Riverside</td>
									<td>UC Merced</td>
								</tr>
							</table>
							Please contact the organizers if you have any
							questions.
						</div>

						
					</section>

					<section>
						<header class="major" id="pcmember">
							<h2>Program Committee</h2>
						</header>
						<ul>
							<li>Zhongliang Chen (AMD)
							</li>
							<li>Xulong Tang (U Pitts)
							</li>
							<li>Wenqian Dong (Florida International University)</li>
							<li>José L. Abellán (UCAM)</li>
							<li>Gunjae Koo (Korea University)</li>
							<li>Hoda Naghibijouybari (Binghamton University)</li>
							<li>Adwait Jog (William & Mary)</li>
							<li>David Kaeli (Northeastern University)
							</li>
							<li>Shi Dong (Cerebras)</li>
						</ul>
					</section>

					<section>
						<header class="major" id="proceedings">
							<h2>Proceedings</h2>
						</header>
							<div class="content">
The accepted papers will be published in the ACM Online Conference Proceedings Series.
							</div>
					</section>

					
				

				<section id="keynote">
					<header id="header">
						<h2>Keynote Speakers</h2>
						<!-- <a class="logo"><strong>
								<font size=6em>Keynotes</font>
							</strong></a> -->
					</header>
					<!-- </section> -->
					<section id="banner">
						<div class="content">
							<div style="float: left; margin-right: 10px;"><img
									src="images/portraits/ucf_99866399.jpg" height="360" /> </div>
							<b>Speaker:</b> Yan Solihin<br />
							<b>Title: TBD  </b><br /><br />
							<p align="justify"><b>Abstract:</b> TBD </p>
							<p align="justify"><b>Bio:</b> Yan Solihin is the Director of Cybersecurity and Privacy Cluster, and Charles N. Millican Chair Professor of Computer Science at University of Central Florida . He obtained B.S. in computer science from Institut Teknologi Bandung in 1995, B.S. in Mathematics from Universitas Terbuka in 1995, M.A.Sc in computer engineering from Nanyang Technological University in 1997, and Ph.D. in computer science from the University of Illinois at Urbana-Champaign (UIUC) in 2002. He is a recipient of 2010 and 2005 IBM Faculty Partnership Award, 2004 NSF Faculty Early Career Award, and 1997 AT&T Leadership Award. He is well known for pioneering cache sharing fairness and Quality of Service (QoS), efficient counter mode memory encryption, and Bonsai Merkle Tree, which have significantly influenced Intel Cache Allocation Technology and Secure Guard eXtension (SGX)'s Memory Encryption Engine (MEE). In recognition, he received IEEE Fellow “for contributions to shared cache hierarchies and secure processors” in 2017. He is listed in the HPCA Hall of Fame, ISCA Hall of Fame, and Computer Architecture Total (CAT) Hall of Fame.</p>
							<br />

						</div>
					</section>

					<section id="banner">
						<div class="content">
							<div style="float: left; margin-right: 10px;"><img
									src="images/portraits/JohnKim-1.jpg" height="360" /> </div>
							<b>Speaker:</b> John Kim<br />
							<b>Title: TBD  </b><br /><br />
							<p align="justify"><b>Abstract:</b> TBD </p>
							<p align="justify"><b>Bio:</b>John Kim is currently an associate professor in the School of Electrical Engineering at KAIST (Korea Advanced Institute of Science and Technology) in Daejeon, Korea. John Kim received his Ph.D. from Stanford University and B.S/M.Eng from Cornell University. His research interests include computer architecture, interconnection networks, security, and mobile systems. Prior to graduate school, he worked on the design of several microprocessors at Intel and at Motorola.</p> 
							<br />
						</div>
					</section>

					<section>
						<header class="major" id="publicity">
							<h2>Publicity</h2>
						</header>
							<div class="content">
								TBD
							</div>
					</section>

					<section>
						<header class="major" id="hist">
							<h2>Workshop Attendance and Impact</h2>
						</header>

						<div class="content">
							<div>
								In general, GPGPU has been one of the highest attended workshops at PPoPP or ASPLOS. Generally, 50-75 people register for the workshop. The average citation count (as per Google Scholar), for a GPGPU workshop paper, is currently ~37.5, where there have been 8 influential papers with 100+ citations.
							</div>
							<br />
						<h2>About Prior GPGPU Workshop Meetings</h2>
						<div>
							David Kaeli (Northeastern) and John Cavazos (Delaware) started this GPGPU workshop series,
which was first held in 2007 at Northeastern University. In 2008, the workshop was held with
ASPLOS 2008. This trend continued and this GPGPU workshop was held with ASPLOS for the
next 6 years. From 2015 to 2018, the GPGPU workshop was co-located with PPoPP. GPGPU
2019 workshop was held with ASPLOS 2019. The last two GPGPU workshops (2020, 2022)
was again co-located with PPoPP.
						</div>
							<h2>Previous versions of the GPGPU workshop:</h2>
							<ul>
								<li><a href="https://sarchlab.github.io/gpgpu2022/" target=&ldquo;blank&rdquo;>GPGPU
										14 (2022)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2022.html"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://insight-archlab.github.io/gpgpu.html"
										target=&ldquo;blank&rdquo;>GPGPU
										13 (2020)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2020.html"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://insight-archlab.github.io/gpgpu12.html"
										target=&ldquo;blank&rdquo;>GPGPU 12
										(2019)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2019.html"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://gpgpu11.000webhostapp.com/" target=&ldquo;blank&rdquo;>GPGPU 11
										(2018)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2018"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://gpgpu10.athoura.com/" target=&ldquo;blank&rdquo;>GPGPU 10
										(2017)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2017"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://conf.researchr.org/track/PPoPP-2016/GPGPU-2016-papers"
										target=&ldquo;blank&rdquo;>GPGPU 09
										(2016)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2016"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU8/"
										target=&ldquo;blank&rdquo;>GPGPU 08
										(2015)</a> <a href="https://dblp.org/db/conf/ppopp/gpgpu2015"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU6/"
										target=&ldquo;blank&rdquo;>GPGPU 07
										(2014)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2014"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="http://www.ece.neu.edu/groups/nucar/GPGPU/GPGPU6/"
										target=&ldquo;blank&rdquo;>GPGPU 06
										(2013)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2013"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2012" target=&ldquo;blank&rdquo;>GPGPU
										05
										(2012)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2012"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2011" target=&ldquo;blank&rdquo;>GPGPU
										04
										(2011)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2011"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2010" target=&ldquo;blank&rdquo;>GPGPU
										03
										(2010)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2010"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li><a href="https://dblp.org/db/conf/asplos/gpgpu2009" target=&ldquo;blank&rdquo;>GPGPU
										02
										(2009)</a> <a href="https://dblp.org/db/conf/asplos/gpgpu2009"
										target=&ldquo;blank&rdquo;>DBLP</a>
								</li>
								<li>GPGPU 01 (2008)
								</li>
							</ul>
						</div>
					</section>



			</div>
		</div>

		<!-- Sidebar -->
		<div id="sidebar">
			<div class="inner">

				<!-- Menu -->
				<nav id="menu">
					<!-- <header class="major">
						<h2>MENU</h2>
					</header> -->
					<ul>
						<li><a href="index.html">Home</a></li>
						<!-- <li><a href="#program">Program</a></li> -->
						<!-- <li><a href="#keynote">Keynotes</a></li> -->
						<li><a href="#importantdates">Important Dates</a></li>
						<li><a href="#guidelines">Submission Guidelines</a>
						</li>
						<li><a href="#organizers">Workshop Organizers</a></li>
						<li><a href="#pcmember">Program Committee</a></li>
						<li><a href="#proceedings">Proceedings</a></li>
						<li><a href="#hist">History and Impact</a></li>

					</ul>
				</nav>
			</div>
		</div>

	</div>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>