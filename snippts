
				<section>
					<header class="major" id="program">
						<h2>Workshop Program</h2>
					</header>
					<div>
						<h3>All times are in US Eastern time (UTC-4). </h3>
						<table class="tg" style="table-layout: fixed; width: 100%">
							<colgroup>
								<col style="width: 160px">
								<col style="width: 730px">
							</colgroup>
							<!-- <tr style="border-bottom: 1px solid #000;border-top: 1px solid #000;">
								<td class="tg-fymr">07:00 AM - 09:00 AM</td>
								<td class="tg-0pky">Breakfast</td>
							  </tr> -->
							<tr style="border-bottom: 1px solid #000;border-top: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">10:00 AM -
										10:10 AM</span></td>
								<td class="tg-0pky">
									Opening Remarks
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/BOZGCt6CxN0"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-0pky"><span style="font-weight:bold">10:10 AM -
										11:10 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Keynote]</span>
									<span style="font-style:italic">Building
										Performant and Portable Heterogenous
										Code using GPU Compute
										Accelerators</span> <br>Derek Bouius,
									AMD
									<a href="#keynote">[Link]</a>
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>This talk will describe the
												various
												methodologies used to offload
												computationally intensive
												workloads
												from CPUs to accelerators. Key
												topics will cover HW
												architecture
												considerations and programming
												methodologies along with
												debugging
												and profiling techniques.</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/00Dnxi4w5uM"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-0pky"><span style="font-weight:bold">11:10 AM -
										11:30 AM</span></td>
								<td class="tg-0pky">Break</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td></td>
								<td></td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr" style="font-weight:bold">
									Session 1</td>
								<td>Session Chair: Daniel Wong</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">11:30 AM -
										11:50 AM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Paper]</span>
									<span style="font-style:italic">Near LLC
										Versus Near Main Memory
										Processing</span><br>Hossein Bitalebi,
									Vahid Geraeinejad, Masoumeh Ebrahimi (KTH
									Royal Institute of Technology)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>Emerging advanced applications,
												such as deep learning and
												graphprocessing, with enormous
												processing demand and massive
												mem-ory requests call for a
												comprehensive processing system
												or ad-vanced solutions to
												address these requirements.
												Near data process-ing is one of
												the promising structures
												targeting this goal.
												However,most recent studies
												have focused on processing
												instructions nearthe main
												memory data banks while
												ignoring the benefits of
												pro-cessing instructions near
												other memory hierarchy levels
												such as LLC. In this study, we
												investigate the near LLC
												processing struc-tures, and
												compare it to the near main
												memory processing alter-native,
												specifically in graphical
												processing units. We analyze
												thesetwo structures on various
												applications in terms of
												performance andpower. Results
												show a clear benefit of near
												LLC processing overnear main
												memory processing in a class of
												applications. Further,we
												suggest a structure, which
												could benefit from both
												structures,requiring the
												applications to be
												characterized in advance or at
												runtime.</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/S7TuzCKK0W0"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">11:50 AM -
										12:10 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Paper]</span>
									<span style="font-style:italic">Accelerating
										Data Transfer between Host and Device
										using Idle GPU</span><br>Yuya Tatsugi,
									Akira Nukada (University of Tsukuba)<br>
									<!-- <br> -->
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>When running single-GPU
												applications on multi-GPU
												compute nodes, the remaining
												GPU devices are kept idle. We
												propose a novel technology to
												accelerate these single-GPU
												applications using the idle GPU
												devices. The data transfers
												between host and device are
												performed not only by the first
												GPU but also by the second GPU
												as well as the alternative
												route with PCI-Express and
												NV-Link connected to it. Our
												performance evaluations show
												the proposed method enables
												about twice data transfer speed
												as native single GPU case for
												large data sizes.</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/7aaO4LJcQz8"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>

							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">12:10 PM -
										12:30 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Invited
										Talk]</span> <span style="font-style:italic"> Towards True
										Coherent Shared Memory for
										Next-generation Multi-GPU
										Systems</span><br>José L. Abellán
									(Universidad Católica de Murcia)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>Multi-GPU (MGPU) systems are
												commonly used today to
												accelerate a variety of
												workloads, including machine
												learning applications, graph
												applications, and large-scale
												simulations. However, the
												inefficiencies in terms of NUMA
												effects and difficulties in
												programming due to the lack of
												hardware coherence in these
												MGPU systems call for new
												architectural solutions for
												MGPU systems. To address these
												inefficiencies, in this talk I
												will present the first proposal
												of a true shared main memory
												(TSM) for MGPU systems, and
												then, to enable seamless
												sharing of data in an MGPU
												system with TSM (MGPU-TSM), I
												will introduce a novel
												lightweight scalable
												timestamp-based coherence
												protocol called MGCC. For
												standard benchmarks, an
												MGPU-TSM system (with 4 GPUs,
												using MGCC) implemented using
												the MGPUSim simulator performs
												on average, 3.7x and 3.0x
												better with relaxed and
												sequential consistency,
												respectively, than the
												non-coherent conventional MGPU
												systems with the same number of
												GPUs. In addition, compared to
												a coherent MGPU system using
												the state-of-the-art HMG
												coherence protocol, an MGPU
												system using MGCC achieves 2.4x
												higher performance. Finally, I
												will discuss some of the key
												challenges and open research
												directions to further optimize
												an MGPU-TSM system. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/wF_5RZZlebI"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">12:30 PM -
										12:50 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Invited
										Talk]</span>
									<span style="font-style:italic">Re-design
										GPU NoC and LLC
										System</span>
									<br>Xia Zhao (Academy
									of Military Sciences)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>To provide high compute
												power, GPUs feature an
												increased number of SMs
												with a larger LLC size and
												higher memory bandwidth.
												Facing the new
												opportunities and
												challenges, how to design a
												scalable and
												high-performance NoC and
												LLC system becomes
												especially important. In
												this talk, I will introduce
												our recent work including
												hierarchy NoC design for
												GPUs, adaptive memory-side
												last-level GPU caching, and
												selective replication in
												memory-side GPU caches.
												The hierarchy NoC provides
												a scalable and low-cost
												interconnect network to
												connect the SMs with the
												LLCs and memory controllers
												by fully exploiting the
												unique traffic pattern in
												GPUs. To solve the
												contention caused by the
												concurrent memory accesses
												sent to the same shared
												data, adaptive LLC can
												adaptively choose shared
												LLC or private LLC based on
												the application
												characteristics. Compared
												to the coarse grain of data
												replication in adaptive
												LLC, selective replication
												can selectively choose the
												replication degree to
												reduce data contention
												while avoiding the high LLC
												miss rate caused by the
												duplicated data. All these
												designs significantly
												increase GPU performance
												for data-intensive
												applications. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/0Lm9URF90co"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">12:50 PM -
										01:10 PM</span></td>
								<td class="tg-0pky">Break</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td></td>
								<td></td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr" style="font-weight:bold">
									Session 2</td>
								<td>Session Chair: Hoda NaghibiJouybari</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">01:10 PM -
										01:30 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Paper]</span>
									<span style="font-style:italic">Systematically
										Extending a High-Level CodeGenerator
										with Support for Tensor Cores</span>
									<br>Lukas Siefke, Bastian Köpcke
									(University of Münster), Michel Steuwer
									(University of Edinburgh), Sergei Gorlatch
									(University of Muenster)
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>High-level code generators like
												Halide, Lift, and RISE make a
												compelling proposition: write
												programs in a simple high-level
												language and get
												high-performing GPU code “for
												free”. They achieve this feat
												by restricting the input
												language to a specific domain
												(such as image and array
												processing in Halide) or to a
												fixed set of flexible parallel
												patterns (as Lift and RISE do).
												Implementing high-level code
												generators that produce
												high-performance code is
												challenging, specifically as
												the target hardware constantly
												evolves.
												In this paper, we discuss how
												we systematically extend the
												RISE high-level code generator
												with support for tensor cores,
												a specialized hardware feature
												of recent Nvidia GPUs. We
												highlight the design of RISE
												that makes it easily extensible
												by following a systematic
												bottom-up approach, that first,
												exposes the imperative tensor
												core API to the code generator,
												then, raises the abstractions
												to an internal low-level
												functional representation,
												that, finally, is targeted by a
												rewrite process that starts
												from a high-level functional
												program.
												Our experimental evaluation
												shows that RISE with support
												for tensor cores generates code
												of competitive performance to
												manually optimized CUDA code,
												which is only up to 36%, but on
												average only 10%, slower than
												Nvidia’s highly optimized
												cuBLAS library, and clearly
												outperforms any code that does
												not exploit tensor cores. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/8IM0sFb_lvw"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>

							</tr>

							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">01:30 PM -
										01:50 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Paper]</span>
									<span style="font-style:italic">Compiler-Assisted
										Scheduling for Multi-Instance
										GPUs</span> <br>Chris Porter (Georgia
									Institute of Technology), Chao Chen (Amazon
									Web Service), Santosh Pande (Georgia
									Institute of Technology)
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>NVIDIA's Multi-Instance GPU
												(MIG) feature allows users to
												partition a GPU's compute and
												memory into independent
												hardware instances. MIG
												guarantees full isolation among
												co-executing kernels on the
												device, which boosts security
												and prevents
												interference-related
												performance degradation.
												Despite the benefits of
												isolation, however, certain
												workloads do not necessarily
												need such guarantees,
												and in fact enforcing such
												isolation can negatively impact
												the throughput of a
												group of processes. In this
												work we aim to relax the
												isolation property for
												certain types of jobs, and to
												show how this can dramatically
												boost throughput
												across a mixed workload
												consisting of jobs that demand
												isolation and others
												that do not. The number of MIG
												partitions is hardware-limited
												but configurable,
												and state-of-the-art workload
												managers cannot safely take
												advantage of unused
												and wasted resources inside a
												given partition. We show how a
												compiler and
												runtime system working in
												tandem can be used to pack jobs
												into partitions when
												isolation is not necessary.
												Using this technique we improve
												overall
												utilization of the device while
												still reaping the benefits of
												MIG's isolation
												properties. Our experimental
												results on NVIDIA A30s with a
												throughput-oriented
												workload show an average of
												1.45x throughput improvement
												and 2.93x increase in
												GPU memory utilization over the
												Slurm workload manager. The
												presented
												framework is fully automatic
												and requires no changes to user
												code. Based on
												these results, we believe our
												scheme is a practical and
												strong advancement over
												state-of-the-art techniques
												currently employed for MIG.
											</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/i2jkFAxt9jE"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
								</td>
							</tr>


							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">01:50 PM -
										02:05 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Work-in-Progress
										Presentation]</span> <span style="font-style:italic"> PTXVM:
										Translating PTX to
										C</span><br>Sreepathi Pai, Benjamin
									Carleton, Benjamin Valpey, Amr Elhelw
									(University of Rochester)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>We describe our ongoing effort
												to translate CUDA PTX kernels
												to C. Our
												translator, PTXVM, generates
												single-threaded C code from
												existing PTX
												kernels and does not need an
												interpreter. PTXVM is
												distinguished by
												its expansive and faithful
												support of NVIDIA's PTX
												specification. This
												enables it to run many complex
												real-life programs such CUB and
												ModernGPU, libraries such as
												cuRAND, and also benchmarks
												such as the
												IrGL graph algorithms, Rodinia,
												and PolyBench. In this talk,
												I'll
												describe the architecture of
												PTXVM as well as an example
												tracing
												infrastructure we've built on
												top of it to gather execution
												statistics. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/aGjZXEEudRE"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">02:05 PM -
										02:20 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Work-in-Progress
										Presentation]</span> <span style="font-style:italic">Understanding
										Wafer-Scale GPU Performance using an
										Architectural Simulator</span><br>Chris
									Thames, Yifan Sun (William &amp; Mary)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>Wafer-Scale chips have the
												potential to break the die-size
												limitation and provide extreme
												performance scalability.
												Existing solutions have
												demonstrated the possibility of
												integrating multi-CPU and
												multi-GPU systems at a
												significantly larger scale on a
												wafer. This increased
												capability results in an
												increase of complexity in
												managing the memory and
												computing resources. To
												facilitate the community study
												wafer-scale systems, this paper
												develops an architectural
												simulator dedicated to model
												wafer-scale multi-device
												systems. Also, this work
												demonstrates analysis of
												initial results from
												simulations on wafer-scale GPU
												systems, providing useful
												insight that can guide future
												system design. </p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/1O7oBIy0GoU"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">02:20 PM -
										02:35 PM</span></td>
								<td class="tg-0pky"><span style="font-weight:bold">[Work-in-Progress
										Presentation]</span> <span style="font-style:italic">ScaleServe: A
										Scalable Multi-GPU Machine Learning
										Inference System and Benchmarking
										Suite</span><br>Ali Jahanshahi, Marcus
									Chow, Daniel Wong (UC Riverside)
									<br>
									<div class="abstract collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Abstract
										</div>
										<div class="collapse-content abstract-content" style="display:none">
											<p>We present, SCALESERVE, a
												scalable multi-GPU inference
												system for a variety of machine
												learning tasks. The proposed
												suite is unique in that each
												component of SCALESERVE
												provides the users with
												configuration knobs which can
												be fine-tuned based on the
												specifications of the
												deployment platform to achieve
												the maximum performance for the
												serving. SCALESERVE also
												provides detailed performance
												metrics/statistics from
												different components of the
												server which can be used by
												designers to characterize the
												bottlenecks of the server.
												We evaluate SCALESERVE serving
												scalability with several
												machine learning tasks
												including computer vision and
												natural language processing on
												an 8-GPU server. We used the
												provided statistic by
												SCALESERVE to fine-tune the
												inference server on our target
												platform to achieve maximum
												performance. The performance
												results for ResNet152 show that
												SCALESERVE is able to scale
												well on a multi-GPU platform.
											</p>
										</div>
									</div>
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/Aem6TcNZJu8"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
							<tr style="border-bottom: 1px solid #000;">
								<td class="tg-fymr"><span style="font-weight:bold">02:35 PM -
										02:40 PM</span></td>
								<td class="tg-0pky">
									Closing Remarks
									<div class=" collapse">
										<div class="collapse-opener">
											<span class="collapse-arrow">
												▶
											</span>
											Recording
										</div>
										<div class="collapse-content" style="display:none">
											<iframe width="560" height="315"
												src="https://www.youtube.com/embed/MAqNlciDO34"
												title="YouTube video player" frameborder="0"
												allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
												allowfullscreen></iframe>
										</div>
									</div>
								</td>
							</tr>
						</table>
					</div>
				</section>


				<section id="keynote">
					<header id="header">
						<a class="logo"><strong>
								<font size=6em>Keynotes</font>
							</strong></a>
					</header>
					<!-- </section> -->
					<section id="banner">
						<div class="content">
							<div style="float: left; margin-right: 10px;"><img
									src="images/portraits/derek-gpgpu2022.png" width="160" height="180" /> </div>
							<b>Speaker:</b> Derek Bouius, AMD<br />
							<b>Title: Building Performant and Portable
								Heterogenous Code using GPU Compute
								Accelerators </b><br /><br />
							<p align="justify"><b>Abstract:</b> This talk will
								describe the various methodologies used to
								offload computationally intensive workloads
								from CPUs to accelerators. Key topics will
								cover HW architecture considerations and
								programming methodologies along with debugging
								and profiling techniques.</p>
							<p align="justify"><b>Bio:</b> Derek Bouius has
								been leading product management of the AMD ROCm
								open software platform for GPU compute for the
								past 4 years. This open source initiative is
								closely tied to enablement of ML and HPC
								workloads using the newest data center GPU
								devices. Derek has been involved in the design,
								development and deployment of security and
								compute accelerators for over 20 years.</p>
							<br />

						</div>
					</section>